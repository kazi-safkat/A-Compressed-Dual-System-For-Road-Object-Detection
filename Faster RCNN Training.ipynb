{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65727abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "device = 'cuda'\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import time\n",
    "import os\n",
    "plt.style.use('ggplot')\n",
    "from tqdm import tqdm\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "from custom_utils import (\n",
    "    Averager, \n",
    "    SaveBestModel, \n",
    "    save_model, \n",
    "    save_loss_plot,\n",
    "    save_mAP\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cb7f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myOwnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.coco = COCO(annotation)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.diction = {1: 0, 2: 1, 3: 2, 4: 3, 6: 4,  7:5, 8: 6, 10: 7, \n",
    "                   11: 8, 12: 9, 17: 10, 37: 11, 73: 12, 77: 13, 79: 14, 'other':14\n",
    "                   }\n",
    "    def __getitem__(self, index):\n",
    "        linix = 1\n",
    "        # Own coco file\n",
    "        coco = self.coco\n",
    "        # Image ID\n",
    "        img_id = self.ids[index]\n",
    "        # List: get annotation id from coco\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        # Dictionary: target coco_annotation file for an image\n",
    "        coco_annotation = coco.loadAnns(ann_ids)\n",
    "        # print(coco_annotation)\n",
    "        # path for input image\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "        # open the input image\n",
    "        img = Image.open(os.path.join(self.root, path))\n",
    "        # number of objects in the image\n",
    "        num_objs = len(coco_annotation)\n",
    "\n",
    "        # Bounding boxes for objects\n",
    "        # In coco format, bbox = [xmin, ymin, width, height]\n",
    "        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n",
    "        labels = []\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            linix = 2\n",
    "            xmin = coco_annotation[i]['bbox'][0]\n",
    "            ymin = coco_annotation[i]['bbox'][1]\n",
    "            xmax = xmin + coco_annotation[i]['bbox'][2]\n",
    "            ymax = ymin + coco_annotation[i]['bbox'][3]\n",
    "            xmin, ymin, xmax,ymax = xmin, ymin, xmax,ymax\n",
    "#              or int(xmax) == int(ymin) or int(xmax) ==int(ymax) or int(xmin)==(ymin) or int(xmin) ==int(ymax)\n",
    "            if int(xmax) == int(xmin) or int(ymin)==int(ymax):\n",
    "                continue\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            try:\n",
    "                labels.append(self.diction[coco_annotation[i]['category_id']])\n",
    "            except:\n",
    "                labels.append(self.diction['other'])\n",
    " \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "#         labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        if linix == 1:\n",
    "            boxes = torch.zeros(0,4)\n",
    "#             labels = torch.zeros((1,),dtype= torch.int64)\n",
    "        # Tensorise img_id\n",
    "        img_id = torch.tensor([img_id])\n",
    "        # Size of bbox (Rectangular)\n",
    "        areas = []\n",
    "        for i in range(num_objs):\n",
    "            areas.append(coco_annotation[i]['area'])\n",
    "        areas = torch.as_tensor(areas, dtype=torch.float32)\n",
    "        # Iscrowd\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        # Annotation is in dictionary format\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = img_id\n",
    "        my_annotation[\"area\"] = areas\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "\n",
    "        return img, my_annotation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e49340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.transforms.Resize((300,300)), torchvision.transforms.Grayscale(num_output_channels=1)\n",
    "def get_transform():\n",
    "    custom_transforms = [  torchvision.transforms.Grayscale(num_output_channels=1)# above is for: randomly selecting one for process\n",
    "        ]\n",
    "    custom_transforms.append(torchvision.transforms.ToTensor())\n",
    "    return torchvision.transforms.Compose(custom_transforms)\n",
    "\n",
    "train_data_dir = 'images_thermal_train'\n",
    "train_coco = 'images_thermal_train/coco.json'\n",
    "val_train_dir = 'images_thermal_val'\n",
    "val_coco = 'images_thermal_val/coco.json'\n",
    "# create own Dataset\n",
    "train_dataset = myOwnDataset(root=train_data_dir,\n",
    "                          annotation=train_coco,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "val_dataset = myOwnDataset(root=val_train_dir,\n",
    "                          annotation=val_coco,\n",
    "                          transforms=get_transform()\n",
    "                          )\n",
    "\n",
    "# collate_fn needs for batch\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# # Batch size\n",
    "train_batch_size = 5\n",
    "val_batch_size = 5\n",
    "# train_dataset = train_dataset.to('cuda')\n",
    "# val_dataset = val_dataset.to('cuda')\n",
    "\n",
    "# # own DataLoader\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=train_batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          collate_fn=collate_fn)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                          batch_size=val_batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b779b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aff6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5922ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i =8\n",
    "img= train_dataset[i][0]\n",
    "bbox = train_dataset[i][1]['boxes']\n",
    "classes = train_dataset[i][1]['labels']\n",
    "for i,j in zip(bbox, classes):\n",
    "    xmin, ymin, xmax, ymax = i\n",
    "    \n",
    "    pt1 = (int(xmin), int(ymin))\n",
    "    pt2 = (int(xmax), int(ymax))\n",
    "    bnd_img = cv2.rectangle(img.permute(1, 2, 0).numpy(),pt1, pt2,(0,0,0),1)\n",
    "    bnd_img = cv2.putText(\n",
    "        bnd_img,\n",
    "        str(j),\n",
    "        (int(xmin), int(ymin) - 10),\n",
    "        fontFace = cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale = 0.3,\n",
    "        color = (0, 255, 255),\n",
    "        thickness=1)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(bnd_img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d88c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # DataLoader is iterable over Dataset\n",
    "# for imgs, annotations in data_loader:\n",
    "#     imgs = list(img.to(device) for img in imgs)\n",
    "#     annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "#     print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a6eee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img, bbox = my_dataset[0]\n",
    "# # img = img.to(torch.int8)\n",
    "# for i,j in zip(bbox[\"boxes\"], bbox[\"labels\"]):\n",
    "#     xmin, ymin, xmax, ymax = i\n",
    "    \n",
    "#     pt1 = (int(xmin), int(ymin))\n",
    "#     pt2 = (int(xmax), int(ymax))\n",
    "    \n",
    "#     bnd_img = cv2.rectangle(img.permute(1, 2, 0).numpy(),pt1, pt2,(0,0,0),2)\n",
    "#     cv2.putText(bnd_img, str(j), (int(xmin-5), int(ymin-2)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,12), 2)\n",
    "\n",
    "# plt.imshow(bnd_img, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d70537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    model.eval()\n",
    "     \n",
    "    # Initialize tqdm progress bar.\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    target = []\n",
    "    preds = []\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(images, targets)\n",
    "\n",
    "        # For mAP calculation using Torchmetrics.\n",
    "        #####################################\n",
    "        for i in range(len(images)):\n",
    "            true_dict = dict()\n",
    "            preds_dict = dict()\n",
    "            true_dict['boxes'] = targets[i]['boxes'].detach().cpu()\n",
    "            true_dict['labels'] = targets[i]['labels'].detach().cpu()\n",
    "            preds_dict['boxes'] = outputs[i]['boxes'].detach().cpu()\n",
    "            preds_dict['scores'] = outputs[i]['scores'].detach().cpu()\n",
    "            preds_dict['labels'] = outputs[i]['labels'].detach().cpu()\n",
    "            preds.append(preds_dict)\n",
    "            target.append(true_dict)\n",
    "        #####################################\n",
    "\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    metric.update(preds, target)\n",
    "    metric_summary = metric.compute()\n",
    "    print(metric_summary)\n",
    "    return metric_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca76b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, RPNHead\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "#     anchor_sizes = ( (32,), (64,), (128,), (256,), (512,))\n",
    "#     aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "#     anchor_generator = AnchorGenerator(\n",
    "#                 anchor_sizes, aspect_ratios\n",
    "#             )\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, weights= 'COCO_V1')\n",
    "#     model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT)\n",
    "    # get number of input features for the classifier\n",
    "#     model.rpn.anchor_generator = anchor_generator\n",
    "#     model.rpn.head = RPNHead(256, anchor_generator.num_anchors_per_location()[0])\n",
    "\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "# 2 classes; Only target class or background\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "\n",
    "def train(train_data_loader, model, optimizer):\n",
    "    print('Training')\n",
    "    model.train()\n",
    "    # optimizer.to('cuda')\n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        # print(data)\n",
    "        optimizer.zero_grad()\n",
    "        # print(data)\n",
    "        images, targets = data\n",
    "        # targets = targets.type(torch.LongTensor) \n",
    "#         print(images[0].shape)\n",
    "        # print(targets)\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.long().to(device) for k, v in t.items()} for t in targets]\n",
    "#         print(targets[0]['boxes'].shape)\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        del images\n",
    "        del targets\n",
    "#         print(\"bbox: \", loss_dict['bbox_regression'])\n",
    "#         print(\"loss dict: \", loss_dict)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        train_loss_hist.send(loss_value)\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss: {loss_value:.4f}\")\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c3d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "multistep = [15,30, 45]\n",
    "learning_rate = 0.0005\n",
    "momentum = 0.9\n",
    "device = 'cuda'\n",
    "# move model to the right device\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "model = get_model_instance_segmentation(16)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "# move model to the right device\n",
    "model.to('cuda')\n",
    "    \n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(train_dataloader)\n",
    "\n",
    "train_loss_hist = Averager()\n",
    "# To store training loss and mAP values.\n",
    "train_loss_list = []\n",
    "map_50_list = []\n",
    "map_list = []\n",
    "\n",
    "# Mame to save the trained model with.\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# Whether to show transformed images from data loader or not.\n",
    "# if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "#     from custom_utils import show_tranformed_image\n",
    "#     show_tranformed_image(train_loader)\n",
    "\n",
    "# To save best model.\n",
    "save_best_model = SaveBestModel()\n",
    "train_loss_hist = Averager()\n",
    "# To store training loss and mAP values.\n",
    "train_loss_list = []\n",
    "map_50_list = []\n",
    "map_list = []\n",
    "\n",
    "# Mame to save the trained model with.\n",
    "MODEL_NAME = 'model'\n",
    "\n",
    "# Whether to show transformed images from data loader or not.\n",
    "# if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "#     from custom_utils import show_tranformed_image\n",
    "#     show_tranformed_image(train_loader)\n",
    "\n",
    "# To save best model.\n",
    "save_best_model = SaveBestModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d455d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = \"./\"\n",
    "for epoch in range(epochs):\n",
    "        print(f\"\\nEPOCH {epoch+1} of {epochs}\")\n",
    "\n",
    "        # Reset the training loss histories for the current epoch.\n",
    "        train_loss_hist.reset()\n",
    "\n",
    "        # Start timer and carry out training and validation.\n",
    "        start = time.time()\n",
    "        # train_dataloader = train_dataloader.to('cuda')\n",
    "        train_loss = train(train_dataloader, model, optimizer)\n",
    "        metric_summary = validate(val_dataloader, model)\n",
    "        print(f\"Epoch #{epoch+1} train loss: {train_loss_hist.value:.3f}\")   \n",
    "        print(f\"Epoch #{epoch+1} mAP@0.50:0.95: {metric_summary['map']}\")\n",
    "        print(f\"Epoch #{epoch+1} mAP@0.50: {metric_summary['map_50']}\")   \n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "        map_50_list.append(metric_summary['map_50'])\n",
    "        map_list.append(metric_summary['map'])\n",
    "\n",
    "        # save the best model till now.\n",
    "        save_best_model(\n",
    "            model, float(metric_summary['map']), epoch, 'outputs'\n",
    "        )\n",
    "        # Save the current epoch model.\n",
    "        save_model(epoch, model, optimizer)\n",
    "\n",
    "        # Save loss plot.\n",
    "        save_loss_plot(\"./\", train_loss_list)\n",
    "\n",
    "        # Save mAP plot.\n",
    "        save_mAP(\"./\", map_50_list, map_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed9d46f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
